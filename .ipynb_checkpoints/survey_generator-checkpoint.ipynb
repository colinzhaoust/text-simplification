{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_template = \"\"\"\n",
    "\n",
    "\n",
    "<!-- You must include this JavaScript file -->\n",
    "<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "\n",
    "<!-- For the full list of available Crowd HTML Elements and their input/output documentation,\n",
    "      please refer to https://docs.aws.amazon.com/sagemaker/latest/dg/sms-ui-template-reference.html -->\n",
    "\n",
    "<!-- You must include crowd-form so that your task submits answers to MTurk -->\n",
    "\n",
    "\n",
    "<style>\n",
    "\n",
    "cand {\n",
    "    color:#0000FF;\n",
    "}\n",
    "\n",
    "candT {\n",
    "    color:#059008;\n",
    "}\n",
    "\n",
    "candF {\n",
    "    color:#FF0000;\n",
    "}\n",
    "\n",
    "pron {\n",
    "    color:#0000FF;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\n",
    "\n",
    "<crowd-form answer-format=\"flatten-objects\">\n",
    "\n",
    "  <crowd-instructions link-text=\"View instructions\" link-type=\"button\">\n",
    "    <short-summary>\n",
    "      <p>Judge the quality of sentence rewriting that tries to use simpler words</p>\n",
    "    </short-summary>\n",
    "  \n",
    "    <detailed-instructions>\n",
    "      <!-- <h3>Provide more detailed instructions here</h3> -->\n",
    "      <p>Please read the groups of sentences below. Each group contains an original sentence along with some variations of it. \n",
    "        These variations are produced by automatic sentence simplification systems, which aims to rewrite the original sentence using simpler words. \n",
    "        Your job is to grade the quality of the variations。</p>\n",
    "    </detailed-instructions>\n",
    "  \n",
    "    <positive-example>\n",
    "      <p>Original Sentence: they are culturally akin to the coastal peoples of papua new guinea .</p>\n",
    "      <p>Variation 1: it 's culture similar to the people of port of papua. <candT>Simplified</candT></p>\n",
    "      <p>Variation 2: they are culturally akin to the coastal peoples of png. <candF>Not Simplified</candF></p>\n",
    "    </positive-example>\n",
    "  \n",
    "    <!-- <negative-example>\n",
    "      <p>Provide an example of a bad answer here</p>\n",
    "      <p>Explain why it's a bad answer</p>\n",
    "    </negative-example> -->\n",
    "\n",
    "\n",
    "  </crowd-instructions>\n",
    "\n",
    "  <br>\n",
    "  \n",
    "  [META_QUESTION_TOKEN]\n",
    "\n",
    "    \n",
    "</crowd-form> \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ranking_template = \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "<!-- You must include this JavaScript file -->\n",
    "<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "\n",
    "<!-- For the full list of available Crowd HTML Elements and their input/output documentation,\n",
    "      please refer to https://docs.aws.amazon.com/sagemaker/latest/dg/sms-ui-template-reference.html -->\n",
    "\n",
    "<!-- You must include crowd-form so that your task submits answers to MTurk -->\n",
    "\n",
    "\n",
    "\n",
    "<style>\n",
    "\n",
    "cand {\n",
    "    color:#0000FF;\n",
    "}\n",
    "\n",
    "candT {\n",
    "    color:#059008;\n",
    "}\n",
    "\n",
    "candF {\n",
    "    color:#FF0000;\n",
    "}\n",
    "\n",
    "pron {\n",
    "    color:#0000FF;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\n",
    "\n",
    "<crowd-form answer-format=\"flatten-objects\">\n",
    "\n",
    "  <crowd-instructions link-text=\"View instructions\" link-type=\"button\">\n",
    "    <short-summary>\n",
    "      <p>Rank the quality of sentence rewriting that tries to use simpler words</p>\n",
    "    </short-summary>\n",
    "  \n",
    "    <detailed-instructions>\n",
    "      <!-- <h3>Provide more detailed instructions here</h3> -->\n",
    "      <p>Please read the groups of sentences below. Each group contains an original sentence along with some variations of it. \n",
    "        These variations are produced by automatic sentence simplification systems, which aims to rewrite the original sentence using simpler words. \n",
    "        Your job is to rank the quality of the variations。</p>\n",
    "    </detailed-instructions>\n",
    "  \n",
    "    <positive-example>\n",
    "      <p>Original Sentence: they are culturally akin to the coastal peoples of papua new guinea .</p>\n",
    "      <p>Variation 1: it 's culture similar to the people of port of papua. <candT>Rank 1</candT></p>\n",
    "      <p>Variation 2: they are culturally akin to the coastal peoples of png. <candT>Rank 2</candT></p>\n",
    "    </positive-example>\n",
    "  \n",
    "    <negative-example>\n",
    "      <p>Original : It was originally thought that the debris thrown up by the collision filled in the smaller craters .</p>\n",
    "      <p>Variation 1: Originally people thought that the small craters were filled in by the pieces thrown up by the crash. <candF>Rank 2</candF></p>\n",
    "      <p>Variation 2: The craters were filled by thrown up dirt. <candF>Rank 1</candF></p>\n",
    "    </negative-example>\n",
    "\n",
    "\n",
    "  </crowd-instructions>\n",
    "\n",
    "  [META_QUESTION_TOKEN]\n",
    "  \n",
    "</crowd-form> \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "templates = {}\n",
    "templates[\"quality\"] = quality_template\n",
    "templates[\"ranking\"] = ranking_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_collection.json\", encoding=\"|utf-8\") as f:\n",
    "    output_collection = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['wikilarge-test', 'wikilarge-eval', 'wikismall-test', 'wikismall-eval', 'newsela-test', 'newsela-eval'])\n",
      "dict_keys(['Complex', 'Dress', 'Dress-Ls', 'EncDecA', 'Hybrid', 'PBMT-R', 'Reference', 'SBMT-SARI'])\n",
      "One side of the armed conflicts is composed mainly of the Sudanese military and the Janjaweed , a Sudanese militia group recruited mostly from the Afro-Arab Abbala tribes of the northern Rizeigat region in Sudan .\n"
     ]
    }
   ],
   "source": [
    "print(output_collection.keys())\n",
    "print(output_collection[\"wikilarge-test\"].keys())\n",
    "print(output_collection[\"wikilarge-test\"][\"Complex\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_quality = \"\"\"\n",
    "<crowd-card> \n",
    "  \n",
    "  <div>\n",
    "    <h3>Q ${question_num}</h3>  \n",
    "    <h3>Original Sentence</h3>\n",
    "    <p>${original_sent}</p>\n",
    "    <h3>Candidate simplification</h3>\n",
    "    <p>${candidate_sent}</p>\n",
    "    \n",
    "  </div>\n",
    "  <br>\n",
    "\n",
    "  <div>\n",
    "    <p>On a scale of 1-5, how much do you think the target is grammartically correct?</p>\n",
    "    <crowd-slider name=\"G\" min=\"1\" max=\"5\" step=\"1\" pin=\"true\" required></crowd-slider>\n",
    "  </div>\n",
    "\n",
    "  <div>\n",
    "    <p>On a scale of 1-5, how much do you think the target preserves the original meaning?</p>\n",
    "    <crowd-slider name=\"M\" min=\"1\" max=\"5\" step=\"1\" pin=\"true\" required></crowd-slider>\n",
    "  </div>\n",
    "\n",
    "  <div>\n",
    "    <p>On a scale of -2-2, how much do you think the target simplifies the word use?</p>\n",
    "    <crowd-slider name=\"S\" min=\"-2\" max=\"2\" step=\"1\" pin=\"true\" required></crowd-slider>\n",
    "  </div>\n",
    "\n",
    "  <div>\n",
    "    <p>On a scale of -2-2, how much do you think the target simplifies the sentence structure?</p>\n",
    "    <crowd-slider name=\"S\" min=\"-2\" max=\"2\" step=\"1\" pin=\"true\" required></crowd-slider>\n",
    "  </div>\n",
    "  \n",
    "</crowd-card>\n",
    "\n",
    "  <br>\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "meta_ranking = \"\"\"\n",
    "<crowd-card>\n",
    "  <div>\n",
    "  <h3>Q ${question_num}</h3>  \n",
    "  \n",
    "    <h3>Original Sentence</h3>\n",
    "    <p>${original_sent}</p>\n",
    "    <h3>Candidate simplification</h3>\n",
    "    [CAND_LISTING_TOKEN]\n",
    "    \n",
    "  </div>\n",
    "\n",
    "  <p>======================================================================================</p>\n",
    "    \n",
    "\n",
    "  <div>  \n",
    "    If two sentences are the same or very similar (wording or complexity),\n",
    "     you may mark them as a tie by giving them the same ranking. <br>\n",
    "     Please try your best to determine the ranking and onlcy mark sentences as a tie when you really feel like you need to.\n",
    "  </div>\n",
    "\n",
    "  <p>======================================================================================</p>\n",
    "\n",
    "    [CAND_RANKING_TOKEN]\n",
    "\n",
    "</crowd-card>\n",
    "\n",
    "  <br>   \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "meta_examples = {}\n",
    "meta_examples[\"quality\"] = meta_quality\n",
    "meta_examples[\"ranking\"] = meta_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quality_html_for_one_example(meta_example, question_num, original, cand):\n",
    "    \n",
    "    this_example = meta_example.replace(\"${question_num}\", question_num)\n",
    "    this_example = this_example.replace(\"${original_sent}\", original)\n",
    "    this_example = this_example.replace(\"${candidate_sent}\", cand)\n",
    "    \n",
    "    return this_example\n",
    "\n",
    "\n",
    "def generate_ranking_html_for_one_example(meta_example, question_num, original, cands):\n",
    "    \n",
    "    gold_cand_listing = \"\"\"\n",
    "    <p>${num}. ${cand_sent}</p>\n",
    "    \"\"\"\n",
    "    \n",
    "    gold_cand_ranking = \"\"\"\n",
    "    <div>\n",
    "    <p>How do you rank candidate ${num}?</p>\n",
    "    <crowd-slider name=\"${num}\" min=\"1\" max=\"4\" step=\"1\" pin=\"true\" required></crowd-slider>\n",
    "    </div>\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    this_example = meta_example.replace(\"${question_num}\", question_num).replace(\"${original_sent}\", original)\n",
    "    \n",
    "    this_cand_listing = \"\"\n",
    "    this_cand_ranking = \"\"\n",
    "    \n",
    "    for i, cand in enumerate(cands):\n",
    "        temp_cand_listing = gold_cand_listing\n",
    "        temp_cand_ranking = gold_cand_ranking\n",
    "        this_cand_listing += temp_cand_listing.replace(\"${num}\", str(i)).replace(\"${cand_sent}\", cand)\n",
    "        this_cand_ranking += temp_cand_ranking.replace(\"${num}\", str(i))\n",
    "        \n",
    "    this_example = this_example.replace(\"[CAND_LISTING_TOKEN]\", this_cand_listing)\n",
    "    this_example = this_example.replace(\"[CAND_RANKING_TOKEN]\", this_cand_ranking)\n",
    "    \n",
    "    return this_example\n",
    "\n",
    "\n",
    "def generate_survey(dataset, selected_ids, survey_name, templates, meta_examples, scope, mode):\n",
    "    # mode in [\"ranking\",\"quality\"]\n",
    "    template =  templates[mode]\n",
    "    meta_example = meta_examples[mode]\n",
    "    \n",
    "    os.makedirs(\"./survey\", exist_ok=True)\n",
    "    os.makedirs(\"./survey/raw_data\", exist_ok=True)\n",
    "    \n",
    "    survey_f = open('./survey/' + survey_name + '.html', 'w', encoding='utf-8')\n",
    "    \n",
    "    this_survey = template\n",
    "    question_html = \"\"\n",
    "    data_clip = []\n",
    "    \n",
    "    \n",
    "    for qid in selected_ids:\n",
    "        temp_clip = {}\n",
    "        cands = []\n",
    "        for key in scope:\n",
    "            temp_clip[\"id\"] = qid\n",
    "            temp_clip[key] = dataset[key][qid]\n",
    "            if key == \"Complex\":\n",
    "                original = dataset[key][qid]\n",
    "            else:\n",
    "                cands.append(dataset[key][qid])\n",
    "        \n",
    "        temp_clip[\"original\"] = original\n",
    "        temp_clip[\"cands\"] = cands\n",
    "        data_clip.append(temp_clip)\n",
    "    \n",
    "    if mode == \"quality\":\n",
    "        count = 1\n",
    "        for i, clip in enumerate(data_clip):\n",
    "            for j, cand in enumerate(clip[\"cands\"]):\n",
    "                question_html += generate_quality_html_for_one_example(meta_example, str(count), clip[\"original\"], cand)\n",
    "                count += 1\n",
    "            \n",
    "    elif mode == \"ranking\":\n",
    "         for i, clip in enumerate(data_clip):\n",
    "            question_html += generate_ranking_html_for_one_example(meta_example, str(i), clip[\"original\"], clip[\"cands\"])      \n",
    "    \n",
    "    \n",
    "    this_survey = this_survey.replace(\"[META_QUESTION_TOKEN]\", question_html)\n",
    "    survey_f.write(this_survey)\n",
    "    survey_f.close()\n",
    "    \n",
    "    raw_data = {}\n",
    "    raw_data[\"name\"] = survey_name\n",
    "    raw_data[\"data_clip\"] = data_clip\n",
    "    raw_data[\"scope\"] = scope\n",
    "    \n",
    "    with open('./survey/raw_data/' + survey_name + '.json', 'w') as f:\n",
    "        json.dump(raw_data, f)\n",
    "    \n",
    "    return \"Finished\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scope reference\n",
    "# wiki-large: test_scope = [\"Complex\", \"Dress\", \"Dress-Ls\", \"EncDecA\", \"Hybrid\", \"PBMT-R\", \"Reference\", \"SBMT-SARI\"]\n",
    "#             eval_scope = [\"Complex\", \"Dress-Ls\", \"Hybrid\", \"PBMT-R\", \"Reference\", \"SBMT-SARI\"]\n",
    "# wiki-small: small_eval_scope = [\"Complex\", \"Dress-Ls\", \"Hybrid\", \"PBMT-R\", \"Reference\"]\n",
    "#             small_test_scope =  [\"Complex\", \"Dress\", \"Dress-Ls\", \"EncDecA\", \"Hybrid\", \"PBMT-R\", \"Reference\"]\n",
    "# newsela:    scope = [\"Complex\", \"Dress\", \"Dress-Ls\", \"EncDecA\", \"Hybrid\", \"PBMT-R\", \"Reference\"]\n",
    "\n",
    "dataset = output_collection[\"wikilarge-test\"]\n",
    "selected_ids = [0,1,2,3,4,5,6,7,8,9]\n",
    "scope = [\"Complex\", \"Dress-Ls\", \"Hybrid\", \"PBMT-R\", \"Reference\"]\n",
    "generate_survey(dataset, selected_ids, \"test_wikilarge_quality\", templates, meta_examples, scope, \"quality\")\n",
    "generate_survey(dataset, selected_ids, \"test_wikilarge_ranking\", templates, meta_examples, scope, \"ranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn\n",
    "from sklearn import metrics #.cohen_kappa_score\n",
    "import pandas as pd\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = pd.read_csv(\"calculation.csv\", names=[\"cg\",\"cm\",\"cls\",\"css\", \"rg\",\"rm\",\"rls\",\"rss\" ])\n",
    "# print(cal)\n",
    "b_cal = cal[[\"cg\",\"cm\", \"rg\",\"rm\"]].apply(lambda x: x>=3)\n",
    "# print(b_cal)\n",
    "s_cal = cal[[\"cls\",\"css\", \"rls\",\"rss\"]].apply(lambda x: x>=0)\n",
    "# print(s_cal)\n",
    "p_cal = cal[[\"cls\",\"css\", \"rls\",\"rss\"]].apply(lambda x: x>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6551724137931034"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metrics.cohen_kappa_score(cal[\"css\"]+cal[\"cls\"]+cal[\"cm\"]+cal[\"cg\"], cal[\"rss\"]+cal[\"rls\"]+cal[\"rm\"]+cal[\"rg\"])\n",
    "\n",
    "# metrics.cohen_kappa_score(b_cal[\"cm\"], b_cal[\"rm\"])\n",
    "metrics.cohen_kappa_score(s_cal[\"css\"], s_cal[\"rss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28721347895177635, 0.07234031448751416)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.pearsonr(p_cal[\"cls\"], p_cal[\"css\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
